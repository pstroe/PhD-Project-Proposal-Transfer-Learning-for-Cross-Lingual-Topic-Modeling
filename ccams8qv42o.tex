\section{Introduction}
``Digitisation'' is a buzz-word in several research communities in present days. The Digital Humanities are no exception. With the amount of digitised content increasing every day, this abundance of data asks not only for innovative, but also efficient ways of analyses and tools for exploration. Once these tools are in place, digital humanists can use them for their manifold research interests and hence knowledge discovery. As such, digitisation has the potential to serve as a catalyst in research and supports researchers in their undertakings.

I say ``potential'' since digital content alone does not yet facilitate research. In the meantime, digitisation has produced huge data silos. At best, the digitised content is available over a web interface and can thus be browsed quite conveniently. This has the positive effect that scientists nowadays can access their data online. However, dedicated systems to analyse and explore the data are missing. The research questions of scientists are often more complex and hence basic search functions do not suffice.

This calls for elaborate information retrieval (IR) systems which assist scientists in conducting their research. Of course, the functionalities of such systems depend on the data to be analysed. Currently, the most often digitised data are textual data. Libraries and archives play a key role here. They have collected huge amounts of historical documents over the past centuries. The majority of these collections are stored in shelves, doomed to fall into oblivion. Nevertheless, or rather therefore, more and more of these collections are nowadays digitised. In a way, they act as gatekeepers and their digitisation strategy influences which information is made publicly available or otherwise accessible online.  

Among written texts, it is mainly books which are digitised. However, there is another text category which currently undergoes massive digitisation processes: newspapers \citep{lansdall-welfarecontent2017,prestondigital2016,binghamdigitization2010}. There are several reasons why we see more and more libraries and archives digitising their material. One is the preservation of delicate documents, yet another is to make certain data more popular, or more easily accessible for different user groups. This proves to be a valuable opportunity for scholars in the digital humanities, like e.~g.~historians, sociologists, and also linguists. Newspaper texts as a scientific resource are of equal importance to each of the aforementioned group of scholars. Newspapers can be seen as a mirror of society. They report daily not only about local, but also about global events and are part of the cultural, social, and political heritage. Most newspaper archives cover several centuries. As such, they serve as an important primary source for historians who want to gain new insights on our past, for sociologists who wish to shed light on social movements, or for linguists who seek to examine how language has changed over time.

In order to allow researchers to work with the digitised data, it must be carefully processed. There exist several information retrieval techniques to analyse the data. Newspapers are particularly interesting texts to analyse since they are already structured by nature. Most newspapers follow a categorisation scheme which assigns each article to a category like ``Economics'' or ``International Affaires''. This convention was not common in the 19th century, though, where newspapers rather tended to publish news as they received them. The challenge we thus face today is to index data for researchers in the humanities so they can easily find content they are interested in. 

One technique that has gained popularity in the digital humanities over the recent years and is used in order to discover the underlying structure of textual data is topic modeling. Topic modeling is an unsupervised generative modeling method which finds topic distributions in documents. These topic distributions are characterised by word distributions, hence each topic consists of a collection of words which should describe a topic. As such, and this is an often stated need by historians, having identified a relevant article, topic models allows the researcher to find similar pages or articles based on the search result.

Applying topic models in the digitial humanities has some shortcomings: 

\begin{enumerate}
    \item Errors from Optical Character Recognition (OCR) directly influence the results. There are either topics which are dominated by them, or they distort the word distributions in a way that they pretend to characterise a topic.
    \item Topic modeling algorithms require a parameter for the number of topics to be inferred from the text. The number of topics can vary from one newspaper collection to another. This would require us to train a topic model for each collections, which is a time consuming undertaking. Moreover, the choice about the number of topics determines how fine or coarse grained a search in a collection can be. The number of topics one want to look at thus directly depends on the researcher working with the texts.
    \item When working with newspaper collections in different languages, we would need to train a topic model for each language, which again is very costly.
    \item Although not restricted to the digital humanities, there are no concrete measures to assess the quality of topic models. What is commonly done is measuring the model perplexity, which does not say anything about the quality of the inferred topics.
\end{enumerate}


\section{Background and State-of-the-Art}

\subsection{Topic modeling}
\paragraph{General topic modeling}Topic modeling has its origins in the research area of information retrieval. Driven by need for indexing documents more reliable and therefore making keyword search more precise, \citet{deerwesterscottindexing1990} proposed Latent Semantic Indexing (LSI). LSI projects documents into a high-dimensional vector space via singular value decomposition, based on the words that occur in the documents. In this vector space, the cosine similarity for documents with comparable content is low and the documents thus are close to each other. Probabilistic Latent Semantic Indexing \citep{hofmannprobabilistic1999} is an extension of LSI. In contrast to Deerwester et.~al's model, words are generated from multinomial random variables, which can be interpreted as topics. As such, it describes a proper generative model and outperforms general LSI. Hofmann's method, however, did not provide a probabilistic model at the level of documents, which is why  \citet{bleilatent2003} proposed Latent Dirichlet Allocation (LDA). In addition to placing a distribution over topics, \citet{bleilatent2003} also use a probabilistic model at the level of documents, which allows them to model the topic distributions in documents. In this way, topic models using LDA belong to the directed graphical models, where topics represent the latent, i.~e.~hidden, variable to be inferred.

In the aftermath of introducing LDA, two approaches directly addressed some limitations. Standard LDA models are incapable of modeling correlations between topics. It is obvious, for example, that a newspaper article about sports is more likely about football than genetics. This is why \citet{Blei:2005:CTM:2976248.2976267} introduced the Correlated Topic Model, which draws the topic distribution from a logistic normal instead a Dirichlet. This gives the topic model more expressive power. Yet another weakness of traditional LDA is that it is unable to treat change in sequential data, to which newspapers belong. More concretely, topics change over time, and the vocabulary which talks about war at the beginning of the 19th century might have significantly changed from the vocabulary used nowadays. In order to capture the evolution of topics, \citet{bleidynamic2006} presented Dynamic Topic Models, which make use of state space models. This type of models calculates topic models for a predefined number of time slices, therefore taking into account the order of documents. When drawing topic proportions for a subsequent time slice, the preceding parameters are used to estimate whether a topic distribution has changed or not. Blei et al. were able to produce more accurate predictive models, and they successfully geared LDA for the exploration of large and poorly structured document collections. This makes the use of Dynamic Topic Models all the more useful for newspaper data. 

There are other techniques than LDA for topic modeling. \citet{EisensteinAX11} highlight three shortcomings of LDA: inference cost, overparametrisation, and lack of sparsity and present an alternative called Sarse Additive Generative (SAGE) model. The key idea is to use  replace the topics with deviations from a background word distribution. As such, topics are characterised by a smaller number of words than is the case for LDA. Moreover, it facilitates the incorporation of other facets, like the time a document was written, sentiment, or syntax, making SAGE models multi-faceted models. Last but not least, a SAGE model turns out to be more efficient than LDA. Eisenstein et al.~found that SAGE's topic are more robust, since it focuses on high-frequency terms with accurate counts, while LDA tends to assign more topical variation to the lowest frequency terms. As such, SAGE models are an alternative which must be considered when implementing topic models.

\paragraph{Word embeddings for topic modeling} Many techniques in natural language processing (NLP) profit from the emergence of word embeddings \citep{mikolovdistributed2013,bengioneural2003}. Word embeddings capture lexico-semantic regularities in language and project similar words close to each other in the embedding space. Ideally, words which describe a topic are semantically related. Hence, including information from word embeddings, into topic models can lead to more coherent topics. \citet{dasgaussian2015} successfully incorporate word embeddings into LDA and report that their technique outperforms standard LDA especially when dealing with out-of-vocabulary words. Instead of calculating the topic distributions over opaque word types, Das et al. replace the categorical distribution with multivariate Gaussian distribution on the embedding space.

\paragraph{Deep learning for topic modeling} With the integration of word embeddings into standard LDA methods, (deep) neural network techniques find their way into the field of topic modeling. \citet{ganscalable2015} introduce Deep Poisson Factor Analysis, which employs sigmoid belief networks (SBM) and restricted Boltzmann machines (RBM) in order to assign topics to text. In contrast to LDA, RBM are undirected graphical models, while SBM are still directed models. They find that their methods are superior to topic modeling using LDA. \citet{cao_novel_2015} propose the feedforward Neural Topic Model, in which they use a topic layer to represent the different topic distributions in regular topic modeling. Their approach is superior to standard LDA since they can directly model \textit{n}-grams via an embedding layer. This enables us to model topics based on other than simple bag-of-word models. For example, the \textit{n}-gram ``White House'' hints at the United States, and possibly has to do with politics. In a bag-of-words approach, the relation to politics cannot be established. Moreover, we would like the name ``Barack Obama'' to occur once in our topic and not as two separate tokens. As such \textit{n}-grams can be viewed as capturing semantic short distance dependencies. In a text, however, the global semantic meaning depends on all the preceding words. \citet{DiengWGP16} use recurrent neural networks (RNN), which are able to grasp long distance relationships. They view topic modeling as a language generation task, in which they try to encode the input in a way that a specific hidden layer represents topics, which are then used in a decoding step to produce the text which served as an input in a variational auto-encoder fashion. \citet{miao_neural_2016}  and \citet{CardTS17} (although rather inspired by SAGE) take a similar approaches and also report lower perplexity scores for their neural topic models.

\paragraph{Topic modeling vs. document modeling} At this point, it is worthwhile to make a distinction between topic modeling and document modeling. \citet{miao_neural_2016} and \citet{CardTS17} deviate from the practice of modeling topics explicitly and thus break with the assumption that a document in a collection consists of a mixture of topic distributions. In such a model, the latent variables can still be interpreted as topics, but a concrete distribution of topics over words is missing. To summarise, the trend seems to go towards modeling topics as hidden layers in generative models using variational auto-encoders. This means that we do not need to model topic distribution over words anymore \citep{Srivastava2016NeuralVI}, but that we rather perform document modeling in a generative manner \citep{MiaoGB17}.

\paragraph{Measuring the quality of topic models}
Since topic modeling is an unsupervised method, the evaluation of topic models is not as straightforward as for supervised information retrieval techniques. Since topic models belong to the generative kind of models, perplexity is used in almost all studies for evaluation. Perplexity for topic models how precisely a topic model is able to generate text it has not seen during training. Apart from perplexity, the quality of topic models can also be assessed in a qualitative manner. \citet{NIPS2009_3700} compared traditional methods for topic modeling evaluation against human judgement via word intrusion and topic intrusion tasks and found that ``[t]raditional metrics are [...] negatively correlated with the measures of topic quality'' (p. 8). They conclude that the evaluation of topic models is indeed task specific and that traditional measures do not represent topic quality accurately.

\paragraph{Multilingual topic modeling}
Given a multilingual newspaper collection, users would not only want to find articles or pages in a single language, but possibly from other languages in order to gain more material on their subject. The challenge here is to jointly model topics in several languages, so they are represented by a common topic distribution which subsequently can be indexed and eventually searched for. Most approaches developed so far used parallel data to infer cross-lingual topics \citep{dumaisautomatic1997,mimnopolylingual2009,zhangcrosslingual2010,nicross2011,vulicprobabilistic2015}. In these models,  we not only observe one word, but two (for a bilingual topic model) or several (for a polylingual topic model) words for which we infer the common topic distribution. \citet{Ma16b} show how they use bilingual topic models for lexicon extraction from non-parallel data and show increased accuracy over TF-IDF or mixed word embedding methods. In general, cross-lingual topic modeling is not a well researched topic as of yet.

\paragraph{Transfer learning and topic modeling}
The aim of transfer learning is to transfer knowledge from an already trained model to a different domain or a different task. I.~e., one's ability to steer a car helps 

\section{Research Project}


\section{Research Project Plan}
Table \ref{researchplan} presents a detailed version of the research project plan.
\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|llcllllcllllcll|}
\cline{2-16}
 & \multicolumn{3}{c|}{\textbf{2017}} & \multicolumn{5}{c|}{\textbf{2018}} & \multicolumn{5}{c|}{\textbf{2019}} & \multicolumn{2}{c|}{\textbf{2020}} \\ \cline{2-16} 
 & \multicolumn{1}{l|}{\textbf{Sep-Nov}} & \multicolumn{2}{l|}{\textbf{Dec-Feb}} & \multicolumn{1}{l|}{\textbf{Mar-May}} & \multicolumn{1}{l|}{\textbf{Jun-Aug}} & \multicolumn{1}{l|}{\textbf{Sep-Nov}} & \multicolumn{2}{l|}{\textbf{Dec-Feb}} & \multicolumn{1}{l|}{\textbf{Mar-May}} & \multicolumn{1}{l|}{\textbf{Jun-Aug}} & \multicolumn{1}{l|}{\textbf{Sep-Nov}} & \multicolumn{2}{l|}{\textbf{Dec-Feb}} & \multicolumn{1}{l|}{\textbf{Mar-May}} & \textbf{Jun-Aug} \\ \cline{2-16} 
\textbf{Preparation} &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  \\
Literature review & \cellcolor[HTML]{3166FF} & \multicolumn{2}{l}{\cellcolor[HTML]{3166FF}} & \cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF} & \multicolumn{2}{l}{\cellcolor[HTML]{ECF4FF}} & \cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF} & \cellcolor[HTML]{ECF4FF} & \multicolumn{2}{l}{\cellcolor[HTML]{ECF4FF}} & \cellcolor[HTML]{ECF4FF} &  \\
Investigate/Implement current methods & \cellcolor[HTML]{3166FF} & \multicolumn{2}{l}{\cellcolor[HTML]{3166FF}} &  &  &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  \\
\textbf{Research Tasks} &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  \\
Topic modeling with word embeddings &  & \multicolumn{2}{l}{} & \cellcolor[HTML]{34FF34} & \cellcolor[HTML]{34FF34} &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  \\
Topic modeling with deep learning &  & \multicolumn{2}{l}{} &  &  & \cellcolor[HTML]{34FF34} & \multicolumn{2}{l}{\cellcolor[HTML]{34FF34}} &  &  &  & \multicolumn{2}{l}{} &  &  \\
Relational topic modeling &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} & \cellcolor[HTML]{34FF34} & \cellcolor[HTML]{34FF34} &  & \multicolumn{2}{l}{} &  &  \\
Cross-linguistic topic modeling &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  & \cellcolor[HTML]{34FF34} & \multicolumn{2}{l}{\cellcolor[HTML]{34FF34}} & \cellcolor[HTML]{34FF34} &  \\
Historical/Dynamic Topic Modeling & & & & & \cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} &\cellcolor[HTML]{34FF34} & \\
\textbf{Milestones} &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  \\
Paper drafts &  & \multicolumn{2}{c}{} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{X}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{2}{c}{\textbf{X}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{X}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{2}{c}{\textbf{X}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c|}{} \\
Annual reports &  & \multicolumn{2}{l}{} &  &  & \textbf{X} & \multicolumn{2}{l}{} &  &  & \textbf{X} & \multicolumn{2}{l}{} &  &  \\
Dissertation compilation &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} &  &  &  & \multicolumn{2}{l}{} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\ \cline{2-16} 
\end{tabular}%
}
\caption{Research Project Plan: An overview of the research program proposed.}
\label{researchplan}
\end{table}

\section{Research Project Details}

\paragraph{Research Project Communication and Deliverables:}
The majority of the outputs of this research plan will be published in journals and at conferences (see the research project plan in Table \ref{researchplan}). The aim is to publish at A-level conferences or in A-level journals. The following deliverables can be defined (non-exhaustive list):

\begin{itemize}
	\item Topic model of newspaper articles which can be integrated in the search engine which will be realised within the Sinergia project
	\item Software to apply (historical) topic modeling on multilingual text collections.
	\item Software relying on relational topic modeling to build a document network.
\end{itemize}

\paragraph{Data Management:}
As detailed in the Sinergia proposal, the EPFL will be responsible for the management of the data. Where licensing permits, the candidate will make new data sets and tools available through his website and/or Github. In addition, we will also release the data set, both in normalised an enriched versions.

\paragraph*{Skills Audit:}
A skills audit for the skills required in this project is shown in Table \ref{skillsaudit}.

\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{Professional and Research Skills}} & \multicolumn{4}{c|}{Rating} & \multicolumn{1}{l|}{\multirow{2}{*}{Evidence}} & \multicolumn{1}{l|}{\multirow{2}{*}{Plan for acquisition}} \\ \cline{2-5}
\multicolumn{1}{|l|}{} & none & basic & competent & proficient & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \hline
\begin{tabular}[c]{@{}c@{}}Understanding of mathematics\\ required for this area\\ (Probability, Linear Algebra,\\ Calculus)\end{tabular} &  & X &  &  & \begin{tabular}[c]{@{}c@{}}Completion of introductory\\ calculus course\end{tabular} & \begin{tabular}[c]{@{}c@{}}Aim must be to move\\ to competent. Taking\\ self-study courses\\ on Coursera or edX.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Using of programming languages\\ for this project (Python,\\ probably Scala)\end{tabular} &  &  & X &  & \begin{tabular}[c]{@{}c@{}}Completion of programming\\ courses related to\\ Computational Linguistics,\\ plus additional self-tuition\end{tabular} & No action required \\ \hline
\begin{tabular}[c]{@{}c@{}}Knowledge and\\ understanding of\\ topic modeling\end{tabular} &  & X &  &  & Introductory MA course & \begin{tabular}[c]{@{}c@{}}Read up on recent\\ literature, aim must\\ be proficient (by the\\ end of this research\\ project)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Knowledge and\\ understanding of\\ machine learning\end{tabular} &  & X &  &  & Introductory MA course & \begin{tabular}[c]{@{}c@{}}Take a machine\\ learning course (UZH,\\ ETH, or self-tuition).\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Knowledge and\\ understanding of\\ deep learning\end{tabular} &  & X &  &  & Participation in Reading Group & \begin{tabular}[c]{@{}c@{}}Take a deep\\ learning course (ETH)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Understanding and \\ application of data\\ collection and\\ analysis methods\end{tabular} &  &  & X &  & \begin{tabular}[c]{@{}c@{}}Participation in Text+Berg\\ and Bulletin4Corpus projects\end{tabular} & No action required. \\ \hline
\end{tabular}%
}
\caption{Skills audit}
\label{skillsaudit}
\end{table}

\section{Research Training}
The research training plan in the following subsection serves as an overview on planned activities in order to obtain the required ECTS (12 in total). The plan is to fulfil this requirement as early as possible, such that in the following, the focus is on the research plan itself.

\subsection{Research Training Plan}
The research training plan with the planned activities I plan for my PhD research follows in Table \ref{researchtraining}.

\begin{table}
\label{researchtraining}
\centering
\begin{tabular}{|l|l|c|c|}
\hline
Date       & Activity                                                                                                   & \multicolumn{1}{l|}{Training} & \multicolumn{1}{l|}{Milestone} \\ \hline
15/08/2017 & Research Proposal                                                                                          & \multicolumn{1}{l|}{}         & X                              \\ \hline
01/09/2017 & Start of the project                                                                                       &                               &                                \\ \hline
19/09/2017 & Kolloquium (3 ECTS)                                                                                        & X                             & \multicolumn{1}{l|}{}          \\ \hline
25/09/2017 & Deep Learning course ETH (4 ECTS)                                                                          & X                             &                                \\ \hline
05/10/2017 & \begin{tabular}[c]{@{}l@{}}Publish or perish: Designing\\ research for publication (1 ECTS)\end{tabular}   & X                             &                                \\ \hline
06/11/2017 & \begin{tabular}[c]{@{}l@{}}Scientific publication: discover, manage\\ \& disseminate (1 ECTS)\end{tabular} & X                             & \multicolumn{1}{l|}{}          \\ \hline
28/11/2017 & \begin{tabular}[c]{@{}l@{}}Resource-focused stress\\ management (1 ECTS)\end{tabular}                      & X                             &                                \\ \hline
SS18       & GRC course 1 (1 ECTS)                                                                                      & X                             &                                \\ \hline
SS18       & GRC course 2 (1 ECTS)                                                                                      & X                             &                                \\ \hline
01/03/2018 & \begin{tabular}[c]{@{}l@{}}Confirmation of candidature\\ (Doktoratsvereinbarung)\end{tabular}              &                               & X                              \\ \hline
01/09/2018 & Annual report year 1                                                                                       & \multicolumn{1}{l|}{}         & X                              \\ \hline
01/09/2019 & Annual report year 2                                                                                       & \multicolumn{1}{l|}{}         & X                              \\ \hline
01/03/2020 & Dissertation draft                                                                                         & \multicolumn{1}{l|}{}         & X                              \\ \hline
01/09/2020 & Dissertation submitted for examination                                                                     & \multicolumn{1}{l|}{}         & X                              \\ \hline
\end{tabular}
\caption{Research training plan: planned activities}
\end{table}

\subsection{Working Hours}
The candidate dedicates 100\% of his working hours (25.2 hours per week) within this proposed research program. The supervision of BA or MA projects, as well as the tutoring of lectures or seminars are possible, but restricted to one supervision/teaching assistant post per semester.

\section{Supervision}

\paragraph{Principal \& Coordinating Supervisor: Prof.~Dr.~Martin Volk}

\begin{itemize}
	\item Directing overall research program
	\item Review research outputs
	\item Provide regular feedback, on both overall, and current subproject progress
\end{itemize}

\paragraph{Co-Supervisor: Dr.~Simon Clematide}

\begin{itemize}
	\item Supervise overall research program
	\item Provide expertise in text mining (especiall topic modeling) and deep learning techniques
	\item Provide regular feedback, on both overall, and current subproject process
\end{itemize}